---
title: "Yuji_Hayashi_R"
author: "Yuji Hayashi"
date: "2020/5/21"
output:
  html_document: default
  pdf_document: default
  word_document: 
    keep_md: yes
    toc: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


# The objective of this notebook
My objective in this notebook is to predict the response variable `classe` in the case of `pml-testing.csv` by building a machine learning model which is made with the training data `pml-training.csv`. Finally, I will make predictions of `classe` for 20 users in the case given as Final Assignment of Coursera. For the details, I chose Random Forest model to predict it because the performance is nicer than other models in terms of accuracy for this dataset.


# Import libraries
At first, let's install and import packages needed in my analysis!

```{r import packages}
#install.packages("caret")
#install.packages("kernlab")
#install.packages("e1071")
#install.packages("ISLR")
#install.packages("ggplot2")
#install.packages("PreProcess")
#install.packages(c("data.table", "dplyr"))
#install.packages("dplyr")
#install.packages("randomForest", quite=TRUE)

library("caret"); library("kernlab"); library("e1071");
library("ISLR"); library("ggplot2"); library("PreProcess");
library("dplyr"); library("readr");library("data.table");
library(randomForest)
```


# Check the data

Set working directory and import the assignment datasets I use here.

```{r import data}
setwd("C:/Users/Yuzi/OneDrive/M1_EDHEC/Financial Econometrics/Final Assignment/workspace")

df <- read_csv("pml-training.csv")
df_test <- read_csv("pml-testing.csv")
```


Check the shapes of each dataset.

```{r shape}
dim(df)
dim(df_test)
```

The train dataset is composed of about 20,000 data points, 160 variables.<br>
On the other hand, the test dataset is composed of 20 data points, 160 variables.


See the above and below 5 rows of each dataset.

```{r head tail}
head(df, 5)
tail(df, 5)
head(df_test, 5)
tail(df_test, 5)
```


See the column names. 

```{r columns}
colnames(df)
colnames(df_test)
```

We can see the similar column names. Here, we do not have to know the details. 


Here, see the data types. 

```{r dtypes}
sapply(df, class)
sapply(df_test, class)
```

Fortunately, almost all of variables are numeric. But there are a few columns having character values. I need to change them into numeric or factor values before running a machine learning model. 


As to columns having character values, here check the exact values.

```{r unique_values}
head(Filter(is.character, df), 5)
unique(df$classe)
unique(df$new_window)
```

The response variable `classe` is composed of `A`, `B`, `C`, `D`, `E`.


See the descriptive statistics of the dataset. Generally, it is tough to follow all of statistical values, but this is an important step to grasp the data.

```{r descriptive statistics}
summary(df)
summary(df_test)
```

Although vizualization of the histgrams or distributions is better to be done here, I skip it because there are so many variables


Check the columns having missing values. These missing values should be avoided before running a machine learning model.

```{r missing values}
sapply(df, function(y) sum(is.na(y)))
sapply(df_test, function(y) sum(is.na(y)))
```


# Preprocessing

Next step is to delete the columns having missing values. In the below process, I transform the data for finding missing values and unlist them to create the new dataframe `df` and `df_test`.

```{r delete columns having missing values, message=FALSE, warning=FALSE, paged.print=FALSE}
df %>% lapply(.,anyNA) %>% unlist %>% !.
df <- df %>% select_if(lapply(.,anyNA) %>% unlist %>% !.)
dim(df)

df_test %>% lapply(.,anyNA) %>% unlist %>% !.
df_test <- df_test %>% select_if(lapply(.,anyNA) %>% unlist %>% !.)
dim(df_test)
```

After deleting the columns having missing values, only 60 explanatory variales are still available! Let's use them to build a prediction model.


Here, again check the columns I will use to create a model.

```{r column names}
colnames(df)
colnames(df_test)
```


Here, let's set an index in the dataset. the most left column `id` is better to be used as an index.

```{r set index}
rownames(df) <- df$id
df$id <- NULL

rownames(df_test) <- df_test$id
df_test$id <- NULL
```


Delete the columns `user_name` and `cvtd_timestamp` because they look unrelated to my objective in the prediction.

```{r delete the columns of "user_name", "cvtd_timestamp"}
df = subset(df, select = -c(user_name,cvtd_timestamp))
df_test = subset(df_test, select = -c(user_name, cvtd_timestamp, problem_id))
dim(df)
dim(df_test)
```

The reason why the number of columns in `df` is more than that of `df_test` is that the dataset `df_test` does not have the column `classe` which is the response variable in this analysis.


To make a prediction model, I need to convert character values into factor values. In R language, factor columns are used for classification problems while numeric columns are used for regression problems.

```{r Change character values into factor values}
df$new_window <- as.factor(df$new_window)
df_test$new_window <- as.factor(df_test$new_window)

df$classe <- as.factor(df$classe)

# Adjust the level of df_test to that of df
levels(df_test$new_window) <- levels(df$new_window)
```


# Split the data


Finally, let's prepare for building a machine learning model!

```{r split the dataset}
inTrain <- createDataPartition(y=df$classe, p=0.8, list=FALSE)
training <- df[inTrain,]
testing <- df[-inTrain,]

dim(training)
dim(testing)
```

Although this point is confusing, I split the "train" dataset into "training" and "testing". This "testing" is not the same as "df_test" dataset on the above. This split is for measuring the performance of the model I create.


Let's create a base model. Before parameter tuning or something, let's see the result as soon as possible. If the accuracy is enough to achieve my objective, that is enough! 


# Base model

```{r Base model}
# Define the control
trControl <- trainControl(method = "cv", number = 10, search = "grid")

# Perform training:
rf_classifier = randomForest(training$classe ~ ., data=training, ntree=200, mtry=3, importance=TRUE, maxnodes = 500, trControl=trControl, metrics="accuracy")
rf_classifier
plot(rf_classifier, ylim=c(0,0.36))
legend('topright', colnames(rf_classifier$err.rate), col=1:3, fill=1:3)
```

I just set up `ntree`, `mtry`, `maxnodes`, `trControl`, `metrics` here. They determine how much deep the trees become, how many counts the experiment iterates, and how we evaluate the model as being "good". I did cv 10 times as default setting. The parameter should be determined by grid search, but the model here seems enough to analyze the data I deal with in this experiment because the performance is very nice as I will explain later.


# Predict

Because the base model seems enough fitted to the training data, let's see the accuracy when applicating it to the testing data.

```{r prediction, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
pred = predict(rf_classifier, newdata=testing)
solution <- data.frame(testing = testing, classe_pred = pred)
```


# Accuracy measures

```{r accuracy in testing data}
# confusion matrix
table(pred, testing$classe)

# Accuracy rate
correctAns <- 0
for ( i in 1:nrow( table( pred, testing$classe )))
correctAns <- correctAns + table(pred, testing$classe)[i,i]
correctAns / nrow( testing )
```

In the testing data, the model achieved its high performance; accuracy is98.6% !


# Predict the given test data

Finally, let's apply my accurate model to prediction of the assignment task!

```{r Apply the model to the assignment data}
pred_test = predict(rf_classifier, newdata=df_test)
pred_test
```

I got a full score in the final quiz given on Coursera platform by answering the above results!! C'est parfait!